---
title: "Model Fitting and Model Evaluation"
author: Prasanga Paudel
date: February 28, 2025
format:
  html:
    toc: false
    number-sections: true
    highlight-style: github
---

```{r, echo=FALSE, message=FALSE}
# loading R packages
library(dplyr)
library(ggplot2)
library(here)
library(tidyr)
library(knitr)
library(patchwork)
library(GGally)
library(caret)
library(pROC)
library(tidymodels)
```

# Introduction

This project is completed as a requirement for BIOS: 7010 (Modern Applied Data Analytiics). This “Model Fitting” assessment is a data exploration and model-fitting project where I use real pharmacokinetic (drug concentration) data for a drug called Mavoglurant. Here, we are practicing the full modeling workflow, from data wrangling and EDA (exploratory data analysis) to fitting and evaluating simple statistical models using the tidymodels framework in R.

I will not be not doing a pharmacology study in detail, but the goal is to learn and demonstrate modeling workflow skills.

The objectives for this project are set as:

1.  Load, clean, and explore messy real-world data (with little documentation).

2.  Derive a meaningful outcome variable (“Y”) from time-series drug concentration data.

3.  Fit simple linear and logistic models with tidymodels.

4.  Interpret the results and document the analysis clearly in a Quarto file.

# Importing Dataset

The dataset Mavoglurant_A2121_nmpk.csv contains pharmacokinetic measurements with repeated observations over time for multiple subjects who received different doses of a drug.

We will first import our dataset from the directory and observe the first six observation from the dataset.

```{r}
# Constructing the file path using here()
file_path <- here("fitting-exercise","data", "Mavoglurant_A2121_nmpk.csv")

# Importing the Excel file from the file path
Mavoglurant_data <- read.csv(file_path)

# Viewing the uppermost data of the imported dataset
head(Mavoglurant_data)
```

We can observe that there are some dummy variables such as SEX, categorical varaiables such as RACE, and contunuous variables such as WT, HT, etc. A brief description of some of the interesting variables is provided below:

| Variable | Description |
|------------------------------------|------------------------------------|
| ID | Individual patient/subject ID |
| TIME | Time since dose (hours) |
| DV | Drug concentration in the body (the measured value at that time) |
| DOSE | Amount of drug administered |
| AMT | Dosing amount (nonzero when TIME = 0) |
| OCC | Occasion (e.g., 1st or 2nd dose cycle) |
| AGE, SEX, RACE, WT, HT | Subject characteristics |
| EVID | Event ID (whether a record is a dose event or observation) |

# Looking into the dataset and preparing for EDA

In this section, we will have an initial look into the dataset and prepare a new dataset for Exploratory Data Analysis.

## Plotting DV on Y-axis and TIME on X-axis

As discussed, DV represents the drug concentration in the body (the measured value at a particular time). As this variable is an important part of our analysis, we will look at how the values within this value are distributed. Here, we will write a code to create a plot that shows a line for each individual, with DV on the y-axis and time on the x-axis, stratified by dose (using a different color for each dose).

```{r}
# Creating a line plot with different colors for each dose
ggplot(Mavoglurant_data, aes(x = TIME, y = DV, group = ID, color = factor(DOSE))) +
  geom_line() +
  labs(title = "DV over Time by ID",
       x = "Time",
       y = "DV",
       color = "Dose") +
  theme_minimal()
```

This figure shows the three doses in three colors. The DOSE 37.5 is not visible in this graph, probably because it is less frequent.

Lets actually look into the dataset to see if this is the case. Lets tabulate the DOSE variable basd on frequency.

```{r}
table(Mavoglurant_data$DOSE)
```

As suspected, Dose amount 37.5 is less frequent than other two doses, 25 and 50, which are both above 1200.

## Considering the time stamp

As there are some individuals that seem to have received the drug more than once, we will only keep observations with OCC = 1

```{r}
#writing a code that keeps only observations with OCC = 1
Mavoglurant_data_OCC1 <- subset(Mavoglurant_data, OCC == 1)

#writing code to exclude the observations with TIME = 0
Mavoglurant_data_filtered <- Mavoglurant_data_OCC1 %>%
  filter(TIME != 0)
```

## Computing the total amount of drug for each individual by adding all the DV values

Here, we will compute the total amount of drug for each individual by adding all the DV values. The “Y” variable is the outcome variable which we will try to model. It represents the total drug concentration measured over time for each subject.

Here’s how it’s created:

Each subject (ID) has multiple rows (time points). We remove the “dose event” rows (TIME == 0) because they’re not actual concentration measurements. Then, we sum up all the DV values for each individual across all their time points.

```{r}
summarized_data <- Mavoglurant_data_filtered %>%
  group_by(ID) %>%
  summarize(Y = sum(DV, na.rm = TRUE))

dim(summarized_data) # checking dimension
```

Since, the total number of individuals in the dataset is 120, the dimensions are correct.

Now, we will create a dataframe that contains only the observations where TIME == 0, and then we will check the dimensions to see if it is correctly created.

```{r}
# creating a data frame that contains only the observations where TIME == 0
time_zero_data <- Mavoglurant_data_OCC1 %>%
  filter(TIME == 0)

dim(time_zero_data) # checking dimension
```

The number of observations are same as the other dataset we created a while ago. The dataset is therefore correctly created.

Now, we will create a new dataframe using the join function and then we will check the dimensions again to see if it is correctly created.

```{r}
# using the join function to combine those two data frames
final_data <- time_zero_data %>%
  left_join(summarized_data, by = "ID")

dim(final_data) # checking dimension
```

So the resulting dataset after cleaning will have one row per individual, and Y will represent their total observed drug level (kind of a crude version of “area under the concentration-time curve”).

## Creating the final dataset

Now, we need one more edit before our dataset is ready for analysis. Now, we'll write a code that converts RACE and SEX to factor variables and keeps only these variables: Y,DOSE,AGE,SEX,RACE,WT,HT.

```{r}
final_data_cleaned <- final_data %>% mutate (
  RACE = factor(RACE), SEX = factor(SEX)) %>% select (Y, DOSE, AGE, SEX, RACE, WT, HT)

dim(final_data_cleaned)
```

The dimensions are as expected. Let's quickly save the dataset.

```{r}
saveRDS(final_data_cleaned, here("ml-models-exercise", "data", "data.rds"))
```

# Exploratory Data Analysis

Now, we will go through the Exploratory Data Analysis process. We will create tables, figures, and plots to see if there is any relation between the variables.

## Making some useful summary tables

Here, we create a summary table of all the variables we have in our final dataset.

```{r}

# Calculating summary statistics for variables
summary_stats <- final_data_cleaned %>%
  summarise(across(c(Y, DOSE, AGE, WT, HT), 
                 list(Mean = ~ mean(., na.rm = TRUE),
    Min = ~ min(., na.rm = TRUE),
    Max = ~ max(., na.rm = TRUE),
    SD = ~ sd(., na.rm = TRUE))))

# Reshaping the data to better present it
summary_stats <- summary_stats %>%
  pivot_longer(cols = everything(), 
               names_to = c("Variable", ".value"), 
               names_sep = "_")

# Printing the summary statistics as a  table format
kable(summary_stats, caption = "This table represents the summary statistics of the 5 variables.", align = "c")
```

We can observe that the average age of the individuals is 33 years and the average height and weight are 1.75 m and 82.55 kg respectively.

## Plotting the relationship with outcome of interest (Y)

Here, we will show some scatterplots or boxplots, whichever is suitable, between the main outcome of interest (total drug, Y) and other predictors. We will use box-plot if the variable is categorical.

```{r, message=FALSE}

# Scatterplot: Y vs DOSE
p1 <- ggplot(final_data_cleaned, aes(x = DOSE, y = Y)) +
  geom_point(alpha = 0.6, color = "blue") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Y vs DOSE", x = "DOSE", y = "Y")

# Scatterplot: Y vs AGE
p2 <- ggplot(final_data_cleaned, aes(x = AGE, y = Y)) +
  geom_point(alpha = 0.6, color = "green") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Y vs AGE", x = "AGE", y = "Y")

# Scatterplot: Y vs WT
p3 <- ggplot(final_data_cleaned, aes(x = WT, y = Y)) +
  geom_point(alpha = 0.6, color = "purple") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Y vs WT", x = "WT", y = "Y")

# Scatterplot: Y vs HT
p4 <- ggplot(final_data_cleaned, aes(x = HT, y = Y)) +
  geom_point(alpha = 0.6, color = "orange") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Y vs HT", x = "HT", y = "Y")

# Boxplot: Y vs SEX
p5 <- ggplot(final_data_cleaned, aes(x = SEX, y = Y, fill = SEX)) +
  geom_boxplot() +
  labs(title = "Y vs SEX", x = "SEX", y = "Y") +
  theme(legend.position = "none")

# Boxplot: Y vs RACE
p6 <- ggplot(final_data_cleaned, aes(x = RACE, y = Y, fill = RACE)) +
  geom_boxplot() +
  labs(title = "Y vs RACE", x = "RACE", y = "Y") +
  theme(legend.position = "none")

# Combine all plots into a common canvas
combined_plots <- (p1 + p2) / (p3 + p4) / (p5 + p6)

# Display the combined plots
combined_plots

```

We can observe from the figures above that:

1.  Dose and Y (Total drug) dont have a strong correlation.
2.  Age and Y (Total drug) dont have a strong correlation.
3.  Weight and Y (Total drug) have a negative correlation.
4.  Height and Y (Total drug) have a negative correlation.
5.  Sex 1 on average has higher total drug.

## Plotting the distribution of variables

Now, we will plot the distributions of our variables to make sure they all make sense. We will set the bin according to the values and distribution patterns within the variable

```{r}

# Custom theme for consistent styling
custom_theme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  axis.title = element_text(size = 12),
  axis.text = element_text(size = 10),
  plot.margin = margin(1, 1, 1, 1, "cm")
)

# Histogram for Y
p1 <- ggplot(final_data_cleaned, aes(x = Y)) +
  geom_histogram(binwidth = 10, fill = "blue", color = "black") +
  labs(title = "Distribution of Y", x = "Y", y = "Count") +
  custom_theme

# Histogram for DOSE
p2 <- ggplot(final_data_cleaned, aes(x = DOSE)) +
  geom_histogram(binwidth = 10, fill = "green", color = "black") +
  labs(title = "Distribution of DOSE", x = "DOSE", y = "Count") +
  custom_theme

# Histogram for AGE
p3 <- ggplot(final_data_cleaned, aes(x = AGE)) +
  geom_histogram(binwidth = 5, fill = "purple", color = "black") +
  labs(title = "Distribution of AGE", x = "AGE", y = "Count") +
  custom_theme

# Histogram for WT
p4 <- ggplot(final_data_cleaned, aes(x = WT)) +
  geom_histogram(binwidth = 5, fill = "orange", color = "black") +
  labs(title = "Distribution of WT", x = "WT", y = "Count") +
  custom_theme

# Histogram for HT
p5 <- ggplot(final_data_cleaned, aes(x = HT)) +
  geom_histogram(binwidth = 0.01, fill = "red", color = "black") +
  labs(title = "Distribution of HT", x = "HT", y = "Count") +
  custom_theme

# Bar plot for SEX
p6 <- ggplot(final_data_cleaned, aes(x = SEX)) +
  geom_bar(fill = "skyblue", color = "black") +
  labs(title = "Distribution of SEX", x = "SEX", y = "Count") +
  custom_theme

# Bar plot for RACE
p7 <- ggplot(final_data_cleaned, aes(x = RACE)) +
  geom_bar(fill = "pink", color = "black") +
  labs(title = "Distribution of RACE", x = "RACE", y = "Count") +
  custom_theme

# Combining all plots into a common canvas
combined_plots <- (p1 + p2) / (p3 + p4) / (p5 + p6 + p7)

# Display the combined plots
combined_plots


```

The distribution of the variables make sense in my opinion. Age, Weight, Height are all fairly normally distributed. Total drug has a somewhat uniform distribution. Other categorical variables have an unbalanced distribution among them but it is practically possible if the research is focused on specific group of people.

## Pair-wise plots

Here, we will plot different variables against each other to see any possible relation or pattern between the variables.

*Note: Please ignore the plots of a variable against itself as it does not make any sense.*

```{r}

# Selecting the variables of interest
selected_vars <- final_data_cleaned %>%
  select(Y, DOSE, AGE, WT, HT)  # to nclude only numeric variables for pair plots

# Creating a pair plot
pair_plot <- ggpairs(selected_vars, 
                     title = " Plotting pairs of Numeric Variables",
                     lower = list(continuous = wrap("smooth", alpha = 0.3, color = "blue")),  # Add smooth lines
                     diag = list(continuous = wrap("densityDiag", fill = "orange")),  # Density plots on diagonal
                     upper = list(continuous = wrap("cor", size = 4)))  # Add correlation coefficients

# Disply the pair plot
print(pair_plot)


```

It is visible that HT and WT are strongly positively correlated. Age and WT also show some moderately positive correlation. The variables DOSE, AGE, WT and HT also show a positive/negative correlation pattern with our variable of interest Y in the figures.

## Correlation values among variables

Here, we will create a cross-table matrix that will mathematically show the correlation among our variable of interests.

```{r}
# Creating a correlation plot
cor_plot <- ggcorr(selected_vars, 
                   method = c("pairwise", "pearson"),  # Use Pearson correlation
                   label = TRUE,  # to display correlation values
                   label_size = 4, 
                   color = "grey50", 
                   hjust = 0.75, 
                   size = 3, 
                   layout.exp = 1)

# Display the correlation plot
print(cor_plot)

```

It is visible that HT and WT show the maximum correlation with a correlation coefficient of 0.6. Age and HT also show sloghtly weaker correlation. Dose has no correlation with WT and HT.

# Fitting a model for Y

We will now fit a linear model to the continuous outcome (Y) using the main predictor of interest to be DOSE in one case and using all predictors in another case.

## Fitting a linear model using only DOSE as the predictor

In our first model we will only use DOSE to explain the variation in Y (total drug).

Model 1: Y = $β_0$ + $β_1$ DOSE + $ϵ$

```{r}
# Fitting a linear model using only DOSE as the predictor
model_dose <- lm(Y ~ DOSE, data = final_data_cleaned)

summary(model_dose)
```

The p-value for DOSE is less than 0.05. Therefore we can conclude that dose has a significant effect on Y when only DOSE is included in the model, and the effect is positive. A unit increase in DOSE increases the total observed drug (Y) by approximately 58 units.

The $R^2$ value for the model is 0.51

## Fitting a linear model using all predictor

In our next model, we will add AGE, SEX, RACE, WT and HT in addition to DOSE into our model 1.

Model 2: Y= $β_0$ + $β_1$DOSE + $β_2$ AGE + $β_3$ SEX + $β_4$ RACE + $β_5$ WT + $β_6$ HT + $ϵ$

```{r}
# Fit a linear model using all predictors
model_all <- lm(Y ~ DOSE + AGE + SEX + RACE + WT + HT, data = final_data_cleaned)

summary(model_all)
```

Model 2 also shows a similar result for DOSE even after controlling for other variables/factors. DOSE is statistically significant with a similar magnitude positive effect.

We can also observe that Weight has a significant effect on Y. But majority of the predictors are insignificant in the model. This has lead to an insignificant increase in information to predict the dependent vriable and only around 62% variation in the dependent variable is explained by the model with all predictors. With only DOSE the performance is little weaker, with DOSE explaining around 50% of the variation.

## Manually computing RMSE and R-squared for both models

Here, we will compute the RMSE and R-squared for the both the models manually. We will first define a function that represents the formula to calculate RMSE and $R^2$. Then, we will apply the formulas to both the models.

```{r}
# creating function to compute RMSE and R-squared
compute_metrics <- function(model, data) {
  # Predictions
  predictions <- predict(model, newdata = data)
  
  # Actual values
  actual <- data$Y
  
  # Computing RMSE
  rmse <- sqrt(mean((actual - predictions)^2))
  
  # Compute R-squared
  r_squared <- summary(model)$r.squared
  

  return(list(RMSE = rmse, R_squared = r_squared))
}

# Computing metrics for the DOSE-only model
metrics_dose <- compute_metrics(model_dose, final_data_cleaned)

# Computing metrics for the all-predictors model
metrics_all <- compute_metrics(model_all, final_data_cleaned)

# Creatin a data frame with the results
results_table <- data.frame(
  Model = c("DOSE Only", "All Predictors"),
  RMSE = c(metrics_dose$RMSE, metrics_all$RMSE),
  R_squared = c(metrics_dose$R_squared, metrics_all$R_squared)
)

kable(results_table, caption = "Model Performance Metrics", align = "c")

```

This table summarizes the model performance with Y as dependent variable. "All predictor" model has a lower RMSE and explains the variation in dependent variable better that the "Dose only" model. This makes sense as more predictor variables in the full model explain more of the variation and thus the error sum of square drops compared to DOSE-only model.

# Fitting a Logistic model for SEX

Now, this is completelty rediculous! Predicting someone's sex based on other characteristics is a very strange thing to try. However, this is a requirement for this assignment, so we will go forward and see what results we get.


We will now fit a logistic model to the binary outcome (SEX) using DOSE in one case and using all predictors in another.

## Fiting a logistic model using only DOSE as the predictor

The question here is, can we predict SEX by looking at someone's medication dose? Let's see.

We will rely on the following logit model:

Model 3: $SEX = f(DOSE)$

```{r}
# Ensuring SEX is a binary factor
final_data_cleaned <- final_data_cleaned %>%
  mutate(SEX = factor(SEX))

# Fiting a logistic model using only DOSE as the predictor
logistic_dose <- glm(SEX ~ DOSE, data = final_data_cleaned, family = binomial())
summary(logistic_dose)
```

As expected, we can observe that DOSE does not significantly affect SEX, and we can not describe someone's SEX by just looking at their medication doses.

## Fitting a logistic model using all predictors

We just found out that DOSE cant help up predict someone's SEX. But, what if we add more variables to the model? If we add information on height, weight, race, and age in addition to DOSE, can we then predict someone's SEX accurately?

For this purpose, we will rely on the following logit model:

Model 4: $SEX = f(DOSE, AGE, RACE, WT, HT, Y)$

```{r}
# Fit a logistic model using all predictors
logistic_all <- glm(SEX ~ DOSE + AGE + RACE + WT + HT + Y, data = final_data_cleaned, family = binomial())


summary(logistic_all)
```


Well, we obtain an interesting result. It can observed that height (HT) significantly affect SEX. This kind of makes sense when we think about it in this way: Men and Women usually have a distinct height pattern. Men are usually tall and women are usually short in most of the countries. According to reports, the average height for US adult men is around 5'9" (175 cm) and for US adult women is about 5'4" (163 cm). So, prediction of someone's SEX based on height is quite possible. Except height, all other factors included in the model are statistically insignificant.

Now, we will need check the Accuracy and AUC values to compare the performance of our two models: Model 3 and Model 4.

## Computing accuracy and ROC-AUC for both the models

Here, we will compute accuracy and ROC-AUC for both the models discussed above with SEX as dependent variable.


We will again define functions to compute the metrices.

```{r, message=FALSE}
# Function to compute accuracy and ROC-AUC
compute_metrics_logistic <- function(model, data) {
  # Predict probabilities
  predicted_probs <- predict(model, newdata = data, type = "response")
  
  # Convert probabilities to binary predictions (0 or 1)
  predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
  
  # Actual classes
  actual_classes <- as.numeric(data$SEX) - 1  # Convert factor to numeric (0 or 1)
  
  # Compute accuracy
  accuracy <- mean(predicted_classes == actual_classes)
  
  # Compute ROC-AUC
  roc_auc <- roc(actual_classes, predicted_probs)$auc
  
  # Return metrics
  return(list(Accuracy = accuracy, ROC_AUC = roc_auc))
}

# Compute metrics for the DOSE-only model
metrics_dose <- compute_metrics_logistic(logistic_dose, final_data_cleaned)

# Compute metrics for the all-predictors model
metrics_all <- compute_metrics_logistic(logistic_all, final_data_cleaned)

results_table_logistic <- data.frame(
  Model = c("DOSE Only", "All Predictors"),
  Accuracy = c(metrics_dose$Accuracy, metrics_all$Accuracy),
  ROC_AUC = c(metrics_dose$ROC_AUC, metrics_all$ROC_AUC)
)

# Print the table using kable
kable(results_table_logistic, caption = "Logistic Model Performance Metrics", align = "c")
```

The table above shows the accuracy and ROC-AUC for both the models discussed above with SEX as dependent variable. The AUC value for DOSE only slightly better than a random model. The AUC value for All predictors is close to 1 implying a nearly perfect prediction performance. I personally find this very surprising that we can predict SEX so perfectly.The accuracy for "all predictor" model is also better.

# Model Improvement

In this section, we will try a different approach.
First, we will remove the variable RACE from our dataset, and store our seeding value 1234 as rngseed.

```{r}
data <- subset(final_data_cleaned, select = -RACE) #removing race
rngseed=1234 # saving value, rngseed = 1234
set.seed(rngseed) # setting seed based on rngseed
```

We will now split the data into train data and test data. We will use 25% of the data as test data.

```{r}
# Putting 3/4 (75%) of the data
data_split <- initial_split(data, prop = 3/4) #splitting data

# Create data frames for the two sets:
train_data <- training(data_split) # setting training data
test_data  <- testing(data_split) #setting testing data
```

## Fitting models

We will now define the two models that we will be working on: the model with "all predictors" and the model with only DOSE as predictor.

**Note: Ideally, our test data should never be used in the development process of our model. It is only used after we are done with all model selection and we want to test if the model we selected based on our train data infact work equally great on the never-seen before test data.** But, just for the sake of easy comparision we will use the train/test technique in this project although we already used the whole dataset in earlier models.

With that being said, lets define the two models:

```{r}
m1 <- lm(Y ~ DOSE , data = train_data) # defining model 1
m2 <- lm(Y ~ DOSE + AGE + SEX  + WT + HT, data = train_data) #defining model 2
```

Making predictions using train data:

Then, we make predictions based on these models .

```{r}
predictions_m1 <- predict(m1, newdata = train_data)
predictions_m2 <- predict(m2, newdata = train_data)
```

Computing RMSE using train data:

* RMSE for m1

```{r}
RMSE_m1 <- sqrt(mean((train_data$Y - predictions_m1)^2))
print(paste("RMSE for m1:", RMSE_m1))
```

* RMSE for m2

```{r}
RMSE_m2 <- sqrt(mean((train_data$Y - predictions_m2)^2))
print(paste("RMSE for m2:", RMSE_m2))
```

* RMSE for null model

We will also estimate the null model and calculate it's performance metrics so that we can compare the null model with other models.


```{r}
# Fit the null model (intercept-only model)
null_model <- lm(Y ~ 1, data = train_data)

# Generate predictions (mean of Y)
predictions_null <- predict(null_model, newdata = train_data)

# Extract observed values
observed_values <- train_data$Y

# Compute residuals (difference between observed and predicted)
residuals_null <- observed_values - predictions_null

# Compute RMSE for the null model
RMSE_null <- sqrt(mean(residuals_null^2))

# Print RMSE
print(paste("RMSE for the null model:", RMSE_null))
```

Comparing all three RMSE values: RMSE (all pred) \> RMSE (DOSE) \> RMSE (NULL). Based on the RMSE values, we can conclude that model with "all predictors" is the one that is best fitting.

## Cross-Validation usuing train data

In this section, we will use CV technique (10 folds) to split the train-data into 90%-10% analysis dataset and assessment dataset, and compare the RMSE values. We will use the same seed value (1234) which we earlier stored as rngseed.


Lets first define the seed and CV folds.
```{r}
set.seed(rngseed) # setting seed based on rngseed
folds <- vfold_cv(train_data, v = 10) # creating 10 folds
folds
```


Lets now calculate RMSE based on the 10 folds.

```{r}
# Initializing vectors to store RMSE results
rmse_m1 <- numeric(10)
rmse_m2 <- numeric(10)
analyze_size <- numeric(10)   
assessment_size <- numeric(10)   

# Looping through each fold
for (i in 1:10) {
  # Splitting  the data into analysis and assessment datasets
  analysis_data <- analysis(folds$splits[[i]])  # 90% analysis data
  assessment_data <- assessment(folds$splits[[i]])  # 10% assessment data

  # Storing sample sizes
  analyze_size[i] <- nrow(analysis_data)  
  assessment_size[i] <- nrow(assessment_data) 

  # Fitting Model 1 (m1): Y ~ DOSE
  m1 <- lm(Y ~ DOSE, data = analysis_data)
  predictions_m1 <- predict(m1, newdata = assessment_data)
  rmse_m1[i] <- sqrt(mean((assessment_data$Y - predictions_m1)^2)) #RMSE for m1

  # Fit Model 2 (m2): Y ~ DOSE + AGE + SEX + WT + HT
  m2 <- lm(Y ~ DOSE + AGE + SEX + WT + HT, data = analysis_data)
  predictions_m2 <- predict(m2, newdata = assessment_data)  # Corrected spelling
  rmse_m2[i] <- sqrt(mean((assessment_data$Y - predictions_m2)^2)) #RMSE for m2
}

# Combining results into a data frame
results <- data.frame(
  Fold = 1:10,
  Analyze_Size = analyze_size,  
  Assessment_Size = assessment_size,
  "RMSE DOSE-only" = rmse_m1,
  "RMSE Full" = rmse_m2
)
```

We have now calculated the RMSE values for both the models based on the 10 folds created using the train data. RMSE_m1 represents the RMSE for the dose-only model. RMSE_m2 represents the RMSE value for the all predictors model.


Lets see the actual result over 10 folds.

```{r}
print(results)
```

Lets calculate the average RMSE value over the 10 folds for both the models.


```{r}
# Compute average RMSE for each model
avg_rmse_m1 <- mean(rmse_m1)
avg_rmse_m2 <- mean(rmse_m2)

print(paste("Average RMSE for DOSE-only model:", avg_rmse_m1))
print(paste("Average RMSE for all predictors mdoel:", avg_rmse_m2))
```

On average, the RMSE value for the Dose-only model (m1) is greater than RMSE for the full model (m2).

Now that we have the RMSE values, we can also calculate the standard errors for the models

```{r}
se_rmse_m1 <- sd(rmse_m1) / sqrt(10)
print(paste("Standard Error of RMSE for m1:", se_rmse_m1))
```

The standard Error for model m1 is 67.50

```{r}
se_rmse_m2 <- sd(rmse_m2) / sqrt(10)
print(paste("Standard Error of RMSE for m2:", se_rmse_m2))
```

The standard Error for model m2 is 64.82

The standard error for both the Model are fairly acceptable given the average is around 645-690, and the RMSE across the 10 folds can be considered fairly consistent.

### Changing the random seed to: 1212

Here, we will change the seeding value and do all the analysis again.

```{r}
set.seed(1212) # setting seed based on rngseed
folds <- vfold_cv(train_data, v = 10) # creating 10 folds
folds
```

```{r}
# Initializing vectors to store RMSE results
rmse_m1 <- numeric(10)
rmse_m2 <- numeric(10)
analyze_size <- numeric(10)  
assessment_size <- numeric(10)  

# Looping each fold
for (i in 1:10) {
  # splitting data into analysis and assessment sets
  analysis_data <- analysis(folds$splits[[i]])  # 90% analysis data
  assessment_data <- assessment(folds$splits[[i]])  # 10% assessment data

  # storing sample sizes
  analyze_size[i] <- nrow(analysis_data)  # Corrected variable name
  assessment_size[i] <- nrow(assessment_data)  # Corrected variable name

  # fitting Model 1 (m1): Y ~ DOSE
  m1 <- lm(Y ~ DOSE, data = analysis_data)
  predictions_m1 <- predict(m1, newdata = assessment_data)
  rmse_m1[i] <- sqrt(mean((assessment_data$Y - predictions_m1)^2))

  # fitting Model 2 (m2): Y ~ DOSE + AGE + SEX + WT + HT
  m2 <- lm(Y ~ DOSE + AGE + SEX + WT + HT, data = analysis_data)
  predictions_m2 <- predict(m2, newdata = assessment_data)  # Corrected spelling
  rmse_m2[i] <- sqrt(mean((assessment_data$Y - predictions_m2)^2))
}

# combining results into a data frame
results <- data.frame(
  Fold = 1:10,
  Analyze_Size = analyze_size,  
  Assessment_Size = assessment_size,  
  RMSE_m1 = rmse_m1,
  RMSE_m2 = rmse_m2
)

# printing results
print(results)

# compute average RMSE for each model
avg_rmse_m1 <- mean(rmse_m1)
avg_rmse_m2 <- mean(rmse_m2)

print(paste("Average RMSE for m1:", avg_rmse_m1))
print(paste("Average RMSE for m2:", avg_rmse_m2))
```

In most of the cases, the RMSE of Dose-only model is still higher that the model with all predictors.

m1, on average, has a higher RMSE than m2. This is similar to the result obtained from previous seed. 

```{r}
se_rmse_m1 <- sd(rmse_m1) / sqrt(10)
print(paste("Standard Error of RMSE for m1:", se_rmse_m1))
```

```{r}
se_rmse_m2 <- sd(rmse_m2) / sqrt(10)
print(paste("Standard Error of RMSE for m2:", se_rmse_m2))
```

The Standard Error decreased in both the cases when we use another random seed, more importantly, the order remains the same. Model 2 has a smaller standard error.

# This is a collaboration project, and the following code is where ANNALISE CRAMER's contribution begins:

First, we will combine the non-cross validated models together into a dataframe.

```{r, message=FALSE}
#create data frame with obs and pred values
df_predictions <- rbind(
  data.frame(Observed = train_data$Y, Predicted = predictions_m1, Model = "m1"),
  data.frame(Observed = train_data$Y, Predicted = predictions_m2, Model = "m2"),
  data.frame(Observed = train_data$Y, Predicted = predictions_null, Model = "null_model")
)

#check
head(df_predictions)
```

Then use ggplot to create a figure that plots (as symbols) observed values on the x-axis and predictions (from each of the 3 models, including the null model) on the y-axis. Use a different color and/or a different symbol to differentiate between the 3 model predictions. Alternatively, you can use facets. Let both x and y axes go from 0 to 5000 and add a 45 degree line. For a good model, the points will fall along that line, namely observed and predicted values agree - with some scatter.

This looks a little weird given the data is horizontally clustered, but after examining the models more closely I think it's okay, the dose-only model is based off of 3 possible doses, and the null model is based off one value. None of these models looks particularly good.

```{r, message=FALSE}
ggplot(df_predictions, aes(x = Observed, y = Predicted, color = Model, shape = Model)) + #color and shape by model
  geom_point(alpha = 0.6, size = 2) +  # Add points with transparency
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +  # 45-degree line
  scale_x_continuous(limits = c(0, 5000)) +
  scale_y_continuous(limits = c(0, 5000)) +
  labs(title = "Model comparison", 
       x = "Observed", 
       y = "Predicted",
       color = "Model",
       shape = "Model") +
  theme_minimal()

#exploring data
#plot(m1)
#plot(m2)
#plot(null_model)
```

Our next plot will be for model 2, plotting predicted versus residuals (the latter being just residuals = predicted-observed). Also add a straight line at 0. Make sure your y-axis goes the same amount into the positive and negative direction.

The residuals have a pattern, a general negative to positive diagonal trend. This means the model isn't capturing some aspect of the data.

```{r}
#make a dataframe of only model2
m2df <- df_predictions[df_predictions$Model == "m2", ]
m2df$Residuals <- m2df$Predicted - m2df$Observed

#plot(m2, which = 1) #I have to do it using ggplot

ggplot(m2df, aes(x = Predicted, y = Residuals)) +
  geom_point(color = "blue", alpha = 0.6, size = 2) +  # Scatter points
  geom_hline(yintercept = 0, linetype = "solid", color = "red") +  # Line at y = 0 #red line at 0
  scale_y_continuous(limits = c(-4500, 4500)) +  # Symmetric y-axis
  scale_x_continuous(limits = c(-4500, 4500)) +  # X-axis limit
  labs(title = "Model 2", 
       x = "Observed", 
       y = "Predicted") +
  theme_minimal()
```

On to our next task, focusing on model 2 with 100 boostrapped values based on the training data.

Next, write a loop (or use a map or apply function) to fit the model to each of the bootstrap samples and make predictions from this model for the original training data. Record all predictions (e.g., in an array or a list).

Once you have all your predictions stored, compute the mean and confidence intervals.

Finally, make a figure that plots observed values on the x-axis, and point estimate (obtained from your original predictions on the training data), as well as median and the upper and lower bounds - obtained by the bootstrap sampling and stored in pred on the y-axis. You can for instance use black symbols for original predictions (the point estimate, which is the mean), and some colors to indicate median and lower and upper confidence limits. As above, make sure x- and y-axis are on the same and add a 45 degree line.

This plot of values looks better than in the earlier plots, the values and their 95% CIs look closer to the 45 degree line. It's hard to say if the light blue (original) or dark blue (bootstrapped) values are closer to the line, though, there's many cases of each instance.

```{r}
library(rsample)
set.seed(rngseed) #set seed

dat_bs <- bootstraps(train_data, times = 100) #get 100 bootstrap samples

#create boot to hold strapped values
boot <- matrix(0, nrow = length(dat_bs$splits), ncol = nrow(train_data))

# for each repetition, fit the model, predict, and store answer
for (i in 1:length(dat_bs$splits)) {
  dat_sample <- rsample::analysis(dat_bs$splits[[i]]) #sample for ith iteration
  m2_bs <- lm(Y ~ DOSE + AGE + SEX + WT + HT, data = dat_sample) #fit model to particular sample
  boot[i, ] <- predict(m2_bs, newdata = train_data) #make pred based on training data
}

#obtain CIs at 95% 
preds <- boot |> apply(2, quantile,  c(0.025, 0.5, 0.975)) |>  t()

#make it a dataframe to plot
preds_df <- data.frame(
  Observed = train_data$Y,
  Original_Pred = predict(m2, newdata = train_data),  # Original model predictions
  Lower_CI = preds[, 1],  # Lower confidence interval
  Median_CI = preds[, 2],  # Median
  Upper_CI = preds[, 3]   # Upper confidence interval
)

#plot observed vs. predictions with CIs
ggplot(preds_df, aes(x = Observed)) +
  geom_point(aes(y = Original_Pred), color = "skyblue3", shape = 16) + #original predictions
  geom_point(aes(y = Median_CI), color = "navy", shape = 16) + #median predictions
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), color = "firebrick", width = 0.1, alpha=0.6) + #CIs
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") + #45-degree line
  labs(x = "Observed", y = "Predicted") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 5000), ylim = c(0, 5000)) #axes go from 0 to 5000

```

# This section is agian contributed by PRASANGA PAUDEL (The Author)

Now, as we have performed all testing and evaluations of our models, we will finally thest the performance of our full model with all predictors on test data, which we set apart at the start of section 7.

Defining the model:

```{r}
model_train <- lm(Y ~ DOSE + AGE + SEX + WT + HT, data = train_data)
```

Making predictions based on above model for the test data.

```{r}
test_data$Predicted <- predict(model_train, newdata = test_data)
```

Now , we will plot the graph to show the predicted and observed value for the two datasets: train data and test data. 

```{r}
# adding predictions for the training data 
train_data$Predicted <- predict(model_train, newdata = train_data)

# creating the plot
ggplot() +
  #  training data
  geom_point(data = train_data, aes(x = Y, y = Predicted), color = "blue", size = 3, alpha = 0.7, shape = 16) +  # Training data
  #  test data
  geom_point(data = test_data, aes(x = Y, y = Predicted), color = "red", size = 3, alpha = 0.7, shape = 17) +  # Test data
  #  45-degree line
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +  # 45-degree line
  # Labels and title
  labs(
    x = "Observed Values",
    y = "Predicted Values",
    title = "Predicted vs Observed Values for Training and Test Data",
    subtitle = "Blue: Training Data, Red: Test Data"
  ) +
  # Set axis limits
  scale_x_continuous(limits = c(0, max(c(train_data$Y, test_data$Y)))) +  # Set x-axis limits
  scale_y_continuous(limits = c(0, max(c(train_data$Predicted, test_data$Predicted)))) +  # Set y-axis limits
  # Theme
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  )
```

We can observe that the test data is more or less close to the 45-degree line and can be assumed to follow the same distribution as the training data. Therefore our model performs well in cace of testing data as well.

# Conclusion

In summary,
All our other models have shown to be better than a NULL model.

Although the model with only DOSE gives a better result than the NULL model, the explanation power is rather limited with a single independent variable. As there is very less variation in the predictor itself ( three possible value of DOSE), we cant explain much of the variation in the dependent variable.

Our model with all predictor has shown to be the best result among the three models. We verified that the model performs well in an unseen data (test data) as well. Furthermore, the addition of different factors into the model has enabled the model to adjust for a lot of variation and the prediction has comparatively improved. The difference between the original and predicted values is lowest in case of this model.
