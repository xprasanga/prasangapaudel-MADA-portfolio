---
title: "Model Fitting Exercise"
author: Prasanga Paudel
date: February 28, 2025
format:
  html:
    toc: false
    number-sections: true
    highlight-style: github
---

# This section added by ANNALISE CRAMER, much further down this page


```{r, echo=FALSE, message=FALSE}
# load a few R packages
library(dplyr)
library(ggplot2)
library(here)
library(tidyr)
library(knitr)
library(patchwork)
library(GGally)
library(caret)
library(pROC)
library(tidymodels)
```



# Importing Dataset

We will first import our dataset from the directory.

```{r}
# Constructing the file path using here()
file_path <- here("fitting-exercise","data", "Mavoglurant_A2121_nmpk.csv")

# Importing the Excel file from the file path
Mavoglurant_data <- read.csv(file_path)

# Viewing the uppermost data of the imported dataset
head(Mavoglurant_data)
```




# Looking into the dataset and preparing for EDA 


In this section, we will have an initial look into the dataset and prepare a new dataset for EDA.


## Plotting DV on Y-axis and TIME on X-axis

Here, we will write a code to make a plot that shows a line for each individual, with DV on the y-axis and time on the x-axis, stratified by dose ( using a different color for each dose). 

```{r}
# Creating a line plot with different colors for each dose
ggplot(Mavoglurant_data, aes(x = TIME, y = DV, group = ID, color = factor(DOSE))) +
  geom_line() +
  labs(title = "DV over Time by ID",
       x = "Time",
       y = "DV",
       color = "Dose") +
  theme_minimal()
```

_This figures show the three doses in three colors. The DOSE 37.5 is not visible in this graph, probably because it is less frequent._


## Preparing the data

As there are some individuals that seem to have received the drug more than once, we will only keep observations with OCC = 1

```{r}
#writing a code that keeps only observations with OCC = 1
Mavoglurant_data_OCC1 <- subset(Mavoglurant_data, OCC == 1)

#writing code to exclude the observations with TIME = 0
Mavoglurant_data_filtered <- Mavoglurant_data_OCC1 %>%
  filter(TIME != 0)

```


## Computing the total amount of drug for each individual by adding all the DV values

Here, we will compute the total amount of drug for each individual by adding all the DV values.

```{r}
summarized_data <- Mavoglurant_data_filtered %>%
  group_by(ID) %>%
  summarize(Y = sum(DV, na.rm = TRUE))


dim(summarized_data) # checking dimension
```


Now, we will create a dataframe that contains only the observations where TIME == 0, and then we will check the dimensions to see if it is correctly created. 
```{r}
# creating a data frame that contains only the observations where TIME == 0
time_zero_data <- Mavoglurant_data_OCC1 %>%
  filter(TIME == 0)

dim(time_zero_data) # checking dimension
```

Now, we will create a new dataframe using the join function and then we will check the dimensions to see if it is correctly created. 
```{r}
# using the join function to combine those two data frames
final_data <- time_zero_data %>%
  left_join(summarized_data, by = "ID")


dim(final_data) # checking dimension
```

## Creating the final dataset

Writing a code that converts RACE and SEX to factor variables and keeps only these variables: Y,DOSE,AGE,SEX,RACE,WT,HT
```{r}
final_data_cleaned <- final_data %>% mutate (
  RACE = factor(RACE), SEX = factor(SEX)) %>% select (Y, DOSE, AGE, SEX, RACE, WT, HT
)

dim(final_data_cleaned)
```


# Exploratory Data Analysis

Now, we will go through the Exploratory Data Analysis process. We will create tables, figures, and plots to see if there is any relation between the variables. 

## Making some useful summary tables



Here, we create a summary table of all the variables we have in our final dataset.

```{r}

# Calculating summary statistics for variables
summary_stats <- final_data_cleaned %>%
  summarise(across(c(Y, DOSE, AGE, WT, HT), 
                 list(Mean = ~ mean(., na.rm = TRUE),
    Min = ~ min(., na.rm = TRUE),
    Max = ~ max(., na.rm = TRUE),
    SD = ~ sd(., na.rm = TRUE))))

# Reshaping the data to better present it
summary_stats <- summary_stats %>%
  pivot_longer(cols = everything(), 
               names_to = c("Variable", ".value"), 
               names_sep = "_")

# Printing the summary statistics as a  table format
kable(summary_stats, caption = "Summary Statistics", align = "c")
```

_This table represennts the summary statistics of the 5 variables._



## Plotting the relationship with outcome of interest (Y)


Here, we will show some scatterplots or boxplots, whichever is suitable, between the main outcome of interest (total drug, Y) and other predictors. We will use box-plot if the variable is categorical.


```{r}

# Scatterplot: Y vs DOSE
p1 <- ggplot(final_data_cleaned, aes(x = DOSE, y = Y)) +
  geom_point(alpha = 0.6, color = "blue") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Y vs DOSE", x = "DOSE", y = "Y")

# Scatterplot: Y vs AGE
p2 <- ggplot(final_data_cleaned, aes(x = AGE, y = Y)) +
  geom_point(alpha = 0.6, color = "green") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Y vs AGE", x = "AGE", y = "Y")

# Scatterplot: Y vs WT
p3 <- ggplot(final_data_cleaned, aes(x = WT, y = Y)) +
  geom_point(alpha = 0.6, color = "purple") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Y vs WT", x = "WT", y = "Y")

# Scatterplot: Y vs HT
p4 <- ggplot(final_data_cleaned, aes(x = HT, y = Y)) +
  geom_point(alpha = 0.6, color = "orange") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Y vs HT", x = "HT", y = "Y")

# Boxplot: Y vs SEX
p5 <- ggplot(final_data_cleaned, aes(x = SEX, y = Y, fill = SEX)) +
  geom_boxplot() +
  labs(title = "Y vs SEX", x = "SEX", y = "Y") +
  theme(legend.position = "none")

# Boxplot: Y vs RACE
p6 <- ggplot(final_data_cleaned, aes(x = RACE, y = Y, fill = RACE)) +
  geom_boxplot() +
  labs(title = "Y vs RACE", x = "RACE", y = "Y") +
  theme(legend.position = "none")

# Combine all plots into a common canvas
combined_plots <- (p1 + p2) / (p3 + p4) / (p5 + p6)

# Display the combined plots
combined_plots

```
We can observe from the figures above that: 

1. Dose and Y (Total drug) dont have a strong correlation.
2. Age and Y dont have a strong correlation.
3. Weight and Y have a negative correlation.
4. Height and Y have a negative correlation.
5. Sex 1 on average has higher total drug.

## Plotting the distribution of variables

Now, we will plot the distributions of our variables to make sure they all make sense. We will set the bin according to the values and distribution patterns within the variable 

```{r}

# Custom theme for consistent styling
custom_theme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  axis.title = element_text(size = 12),
  axis.text = element_text(size = 10),
  plot.margin = margin(1, 1, 1, 1, "cm")
)

# Histogram for Y
p1 <- ggplot(final_data_cleaned, aes(x = Y)) +
  geom_histogram(binwidth = 10, fill = "blue", color = "black") +
  labs(title = "Distribution of Y", x = "Y", y = "Count") +
  custom_theme

# Histogram for DOSE
p2 <- ggplot(final_data_cleaned, aes(x = DOSE)) +
  geom_histogram(binwidth = 10, fill = "green", color = "black") +
  labs(title = "Distribution of DOSE", x = "DOSE", y = "Count") +
  custom_theme

# Histogram for AGE
p3 <- ggplot(final_data_cleaned, aes(x = AGE)) +
  geom_histogram(binwidth = 5, fill = "purple", color = "black") +
  labs(title = "Distribution of AGE", x = "AGE", y = "Count") +
  custom_theme

# Histogram for WT
p4 <- ggplot(final_data_cleaned, aes(x = WT)) +
  geom_histogram(binwidth = 5, fill = "orange", color = "black") +
  labs(title = "Distribution of WT", x = "WT", y = "Count") +
  custom_theme

# Histogram for HT
p5 <- ggplot(final_data_cleaned, aes(x = HT)) +
  geom_histogram(binwidth = 0.01, fill = "red", color = "black") +
  labs(title = "Distribution of HT", x = "HT", y = "Count") +
  custom_theme

# Bar plot for SEX
p6 <- ggplot(final_data_cleaned, aes(x = SEX)) +
  geom_bar(fill = "skyblue", color = "black") +
  labs(title = "Distribution of SEX", x = "SEX", y = "Count") +
  custom_theme

# Bar plot for RACE
p7 <- ggplot(final_data_cleaned, aes(x = RACE)) +
  geom_bar(fill = "pink", color = "black") +
  labs(title = "Distribution of RACE", x = "RACE", y = "Count") +
  custom_theme

# Combining all plots into a common canvas
combined_plots <- (p1 + p2) / (p3 + p4) / (p5 + p6 + p7)

# Display the combined plots
combined_plots


```

The distribution of the variables make sense in my opinion. Age, Weight, Height are all fairly normally distributed. Total drug has a somewhat uniform distribution. Other categorical variables have an unbalanced distribution among them but it is practically possible if the research is focused on specific group of people.

## Pair-wise plots

Here, we will plot different variables against each other to see any possible relation or pattern between the variables. 

_Note: Please ignore the plots of a variable against itself as it does not make any sense._

```{r}

# Selecting the variables of interest
selected_vars <- final_data_cleaned %>%
  select(Y, DOSE, AGE, WT, HT)  # to nclude only numeric variables for pair plots

# Creating a pair plot
pair_plot <- ggpairs(selected_vars, 
                     title = " Plotting pairs of Numeric Variables",
                     lower = list(continuous = wrap("smooth", alpha = 0.3, color = "blue")),  # Add smooth lines
                     diag = list(continuous = wrap("densityDiag", fill = "orange")),  # Density plots on diagonal
                     upper = list(continuous = wrap("cor", size = 4)))  # Add correlation coefficients

# Disply the pair plot
print(pair_plot)


```
It is visible that HT and WT are strongly correlated. Y shows correlation with WT. Age and WT also show some moderate correlation.

## Correlation among variables

Here, we will create a cross-table matrix that shows the correlation among our variable of interests.

```{r}
# Creating a correlation plot
cor_plot <- ggcorr(selected_vars, 
                   method = c("pairwise", "pearson"),  # Use Pearson correlation
                   label = TRUE,  # to display correlation values
                   label_size = 4, 
                   color = "grey50", 
                   hjust = 0.75, 
                   size = 3, 
                   layout.exp = 1)

# Display the correlation plot
print(cor_plot)

```

It is visible that HT and WT show the maximum correlation with a correlation coefficient of 0.6. Age and HT also show sloghtly weaker correlation. Dose has no correlation with WT and HT.

# Fitting a model for Y with only DOSE, and ALL predictors 


We will now fit a linear model to the continuous outcome (Y) using the main predictor of interest to be DOSE in one case and using all predictors in another case.

## Fitting a linear model using only DOSE as the predictor


```{r}
# Fitting a linear model using only DOSE as the predictor
model_dose <- lm(Y ~ DOSE, data = final_data_cleaned)

summary(model_dose)
```
We can observe that dose has an insignificant effect on Y.



## Fitting a linear model using all predictor


```{r}
# Fit a linear model using all predictors
model_all <- lm(Y ~ DOSE + AGE + SEX + RACE + WT + HT, data = final_data_cleaned)

summary(model_all)
```
We can observe that SEX2 and Wt has a significant effect on Y. But majority o the predictors are insignificant in the model. This has lead to an insufficient information to predict the dependent vriable and only around 10% variation in the dependent variable is explained by the model with all predictors. With only DOSE the performance is even poorer.



## computing RMSE and R-squared for both models

Here, we will compute the RMSE and R-squared for the both the models discussed above. 


```{r}


# creating function to compute RMSE and R-squared
compute_metrics <- function(model, data) {
  # Predictions
  predictions <- predict(model, newdata = data)
  
  # Actual values
  actual <- data$Y
  
  # Computing RMSE
  rmse <- sqrt(mean((actual - predictions)^2))
  
  # Compute R-squared
  r_squared <- summary(model)$r.squared
  

  return(list(RMSE = rmse, R_squared = r_squared))
}

# Computing metrics for the DOSE-only model
metrics_dose <- compute_metrics(model_dose, final_data_cleaned)

# Computing metrics for the all-predictors model
metrics_all <- compute_metrics(model_all, final_data_cleaned)

# Creatin a data frame with the results
results_table <- data.frame(
  Model = c("DOSE Only", "All Predictors"),
  RMSE = c(metrics_dose$RMSE, metrics_all$RMSE),
  R_squared = c(metrics_dose$R_squared, metrics_all$R_squared)
)

kable(results_table, caption = "Model Performance Metrics", align = "c")

```

_This tables shows the model performance with Y as dependent variable. "All predictor" model has a lower RMSE and explains the variation in dependent variable better that the "Dose only" model._



# Fitting a Logistic model for SEX with only DOSE, and ALL predictors 


We will now fit a linear model to the binary outcome (SEX) using the main predictor of interest to be DOSE in one case and using all predictors in another.


## Fiting a logistic model using only DOSE as the predictor


```{r}
# Ensuring SEX is a binary factor
final_data_cleaned <- final_data_cleaned %>%
  mutate(SEX = factor(SEX))

# Fiting a logistic model using only DOSE as the predictor
logistic_dose <- glm(SEX ~ DOSE, data = final_data_cleaned, family = binomial())
summary(logistic_dose)
```
We can observe that DOSE does not significantly affect SEX.


## Fitting a logistic model using all predictors

```{r}
# Fit a logistic model using all predictors
logistic_all <- glm(SEX ~ DOSE + AGE + RACE + WT + HT + Y, data = final_data_cleaned, family = binomial())


summary(logistic_all)
```
We can observe that HT significantly affect SEX. We will need to check the Accuracy and AUC values to tell about the performance of the model.


## Computing accuracy and ROC-AUC for both the models

Here, we will compute  accuracy and ROC-AUC for both the models discussed above with SEX as dependent variable.

```{r}

# Function to compute accuracy and ROC-AUC
compute_metrics_logistic <- function(model, data) {
  # Predict probabilities
  predicted_probs <- predict(model, newdata = data, type = "response")
  
  # Convert probabilities to binary predictions (0 or 1)
  predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
  
  # Actual classes
  actual_classes <- as.numeric(data$SEX) - 1  # Convert factor to numeric (0 or 1)
  
  # Compute accuracy
  accuracy <- mean(predicted_classes == actual_classes)
  
  # Compute ROC-AUC
  roc_auc <- roc(actual_classes, predicted_probs)$auc
  
  # Return metrics
  return(list(Accuracy = accuracy, ROC_AUC = roc_auc))
}

# Compute metrics for the DOSE-only model
metrics_dose <- compute_metrics_logistic(logistic_dose, final_data_cleaned)

# Compute metrics for the all-predictors model
metrics_all <- compute_metrics_logistic(logistic_all, final_data_cleaned)

results_table_logistic <- data.frame(
  Model = c("DOSE Only", "All Predictors"),
  Accuracy = c(metrics_dose$Accuracy, metrics_all$Accuracy),
  ROC_AUC = c(metrics_dose$ROC_AUC, metrics_all$ROC_AUC)
)

# Print the table using kable
kable(results_table_logistic, caption = "Logistic Model Performance Metrics", align = "c")
```
The table above shows the  accuracy and ROC-AUC for both the models discussed above with SEX as dependent variable. The AUC value for DOSE only slightly better than a random model. The AUC value for All predictors is close to 1 implying a nearly perfect prediction performance. I personally find this very surprising that we can predict SEX so perfectly.The accuracy for "all predictor" model is also better.

# Module 10  (Model Improvement)

First, we will remove the variable RACE from our dataset, and store our seeding value `1234` as `rngseed`.

```{r}
data <- subset(final_data_cleaned, select = -RACE) #removing race
rngseed=1234 # saving value, rngseed = 1234
set.seed(rngseed) # setting seed based on rngseed
```


We will now split the data into `train_data` and `test data`.

```{r}
# Putting 3/4 (75%) of the data
data_split <- initial_split(data, prop = 3/4) #splitting data

# Create data frames for the two sets:
train_data <- training(data_split) # setting training data
test_data  <- testing(data_split) #setting testing data
```

##Fitting models

We will now define the two models that we will be working on:
the model with "all predictors" and the model with only `DOSE` as predictor
```{r}
m1 <- lm(Y ~ DOSE , data = train_data) # defining model 1
m2 <- lm(Y ~ DOSE + AGE + SEX  + WT + HT, data = train_data) #defining model 2
```


**Making predictions:**

Then, we make predictions based on these models.

```{r}
predictions_m1 <- predict(m1, newdata = train_data)
predictions_m2 <- predict(m2, newdata = train_data)
```

**Computing RMSE**

RMSE for m1

```{r}
RMSE_m1 <- sqrt(mean((train_data$Y - predictions_m1)^2))
print(paste("RMSE for m1:", RMSE_m1))
```

RMSE for m2

```{r}
RMSE_m2 <- sqrt(mean((train_data$Y - predictions_m2)^2))
print(paste("RMSE for m2:", RMSE_m2))
```

RMSE for null model

```{r}
# Fit the null model (intercept-only model)
null_model <- lm(Y ~ 1, data = train_data)

# Generate predictions (mean of Y)
predictions_null <- predict(null_model, newdata = train_data)

# Extract observed values
observed_values <- train_data$Y

# Compute residuals (difference between observed and predicted)
residuals_null <- observed_values - predictions_null

# Compute RMSE for the null model
RMSE_null <- sqrt(mean(residuals_null^2))

# Print RMSE
print(paste("RMSE for the null model:", RMSE_null))
```
Comparing all three RMSE values: RMSE ( all pred) > RMSE (DOSE) > RMSE (NULL)
Based on the RMSE values, we can conclude that Model with "all predictors" is the one that is best fitting.



## Setting random seed (1234)

In this section, we will use CV technique to split the `train-data` into 90%-10% analysis dataset and assessment dataset, and compare the RMSE values.

```{r}
set.seed(rngseed) # setting seed based on rngseed
folds <- vfold_cv(train_data, v = 10) # creating 10 folds
folds
```


```{r}
# Initializing vectors to store RMSE results
rmse_m1 <- numeric(10)
rmse_m2 <- numeric(10)
analyze_size <- numeric(10)   
assessment_size <- numeric(10)   

# Looping through each fold
for (i in 1:10) {
  # Splitting  the data into analysis and assessment datasets
  analysis_data <- analysis(folds$splits[[i]])  # 90% analysis data
  assessment_data <- assessment(folds$splits[[i]])  # 10% assessment data

  # Storing sample sizes
  analyze_size[i] <- nrow(analysis_data)  
  assessment_size[i] <- nrow(assessment_data) 

  # Fitting Model 1 (m1): Y ~ DOSE
  m1 <- lm(Y ~ DOSE, data = analysis_data)
  predictions_m1 <- predict(m1, newdata = assessment_data)
  rmse_m1[i] <- sqrt(mean((assessment_data$Y - predictions_m1)^2)) #RMSE for m1

  # Fit Model 2 (m2): Y ~ DOSE + AGE + SEX + WT + HT
  m2 <- lm(Y ~ DOSE + AGE + SEX + WT + HT, data = analysis_data)
  predictions_m2 <- predict(m2, newdata = assessment_data)  # Corrected spelling
  rmse_m2[i] <- sqrt(mean((assessment_data$Y - predictions_m2)^2)) #RMSE for m2
}

# Combining results into a data frame
results <- data.frame(
  Fold = 1:10,
  Analyze_Size = analyze_size,  
  Assessment_Size = assessment_size,
  RMSE_m1 = rmse_m1,
  RMSE_m2 = rmse_m2
)

# Print results
print(results)

```
We can observe that in most of the cases, the RMSE of "NULL Model" is greater.

```{r}
# Compute average RMSE for each model
avg_rmse_m1 <- mean(rmse_m1)
avg_rmse_m2 <- mean(rmse_m2)

print(paste("Average RMSE for m1:", avg_rmse_m1))
print(paste("Average RMSE for m2:", avg_rmse_m2))
```

But, on average, the RMSE value for the NULL Model (m1) is greater than RMSE for m2

```{r}
se_rmse_m1 <- sd(rmse_m1) / sqrt(10)
print(paste("Standard Error of RMSE for m1:", se_rmse_m1))
```

```{r}
se_rmse_m2 <- sd(rmse_m2) / sqrt(10)
print(paste("Standard Error of RMSE for m2:", se_rmse_m2))

```
The standard error for both the Model are fairly acceptable, and the RMSE across the 10 folds can be considered fairly consistent.


## Changing the random seed to: 1212

Here, we will change the seeding value and do all the analysis again.

```{r}
set.seed(1212) # setting seed based on rngseed
folds <- vfold_cv(train_data, v = 10) # creating 10 folds
folds
```


```{r}
# Initialize vectors to store RMSE results
rmse_m1 <- numeric(10)
rmse_m2 <- numeric(10)
analyze_size <- numeric(10)  # Corrected spelling
assessment_size <- numeric(10)  # Corrected spelling

# Loop through each fold
for (i in 1:10) {
  # Split data into analysis and assessment sets
  analysis_data <- analysis(folds$splits[[i]])  # 90% analysis data
  assessment_data <- assessment(folds$splits[[i]])  # 10% assessment data

  # Store sample sizes
  analyze_size[i] <- nrow(analysis_data)  # Corrected variable name
  assessment_size[i] <- nrow(assessment_data)  # Corrected variable name

  # Fit Model 1 (m1): Y ~ DOSE
  m1 <- lm(Y ~ DOSE, data = analysis_data)
  predictions_m1 <- predict(m1, newdata = assessment_data)
  rmse_m1[i] <- sqrt(mean((assessment_data$Y - predictions_m1)^2))

  # Fit Model 2 (m2): Y ~ DOSE + AGE + SEX + WT + HT
  m2 <- lm(Y ~ DOSE + AGE + SEX + WT + HT, data = analysis_data)
  predictions_m2 <- predict(m2, newdata = assessment_data)  # Corrected spelling
  rmse_m2[i] <- sqrt(mean((assessment_data$Y - predictions_m2)^2))
}

# Combine results into a data frame
results <- data.frame(
  Fold = 1:10,
  Analyze_Size = analyze_size,  # Corrected variable name
  Assessment_Size = assessment_size,  # Corrected variable name
  RMSE_m1 = rmse_m1,
  RMSE_m2 = rmse_m2
)

# Print results
print(results)

# Compute average RMSE for each model
avg_rmse_m1 <- mean(rmse_m1)
avg_rmse_m2 <- mean(rmse_m2)

print(paste("Average RMSE for m1:", avg_rmse_m1))
print(paste("Average RMSE for m2:", avg_rmse_m2))
```

In most of the cases, the RMSE of NULL Model is higher that the model with all predictors. This is similar to the result obtained from previous seed. m1, on average, has a higher RMSE.

```{r}
se_rmse_m1 <- sd(rmse_m1) / sqrt(10)
print(paste("Standard Error of RMSE for m1:", se_rmse_m1))
```

```{r}
se_rmse_m2 <- sd(rmse_m2) / sqrt(10)
print(paste("Standard Error of RMSE for m2:", se_rmse_m2))
```

The Standard Error decreased in both the cases when we use another random seed.



The following code is where ANNALISE CRAMER's contribution begins:

First, we will combine the non-cross validated models together into a dataframe.
```{r}
#create data frame with obs and pred values
df_predictions <- rbind(
  data.frame(Observed = train_data$Y, Predicted = predictions_m1, Model = "m1"),
  data.frame(Observed = train_data$Y, Predicted = predictions_m2, Model = "m2"),
  data.frame(Observed = train_data$Y, Predicted = predictions_null, Model = "null_model")
)

#check
head(df_predictions)
```

Then use ggplot to create a figure that plots (as symbols) observed values on the x-axis and predictions (from each of the 3 models, including the null model) on the y-axis. Use a different color and/or a different symbol to differentiate between the 3 model predictions. Alternatively, you can use facets. Let both x and y axes go from 0 to 5000 and add a 45 degree line. For a good model, the points will fall along that line, namely observed and predicted values agree - with some scatter.

This looks a little weird given the data is horizontally clustered, but after examining the models more closely I think it's okay, the dose-only model is based off of 3 possible doses, and the null model is based off one value. None of these models looks particularly good.
```{r}
ggplot(df_predictions, aes(x = Observed, y = Predicted, color = Model, shape = Model)) + #color and shape by model
  geom_point(alpha = 0.6, size = 2) +  # Add points with transparency
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +  # 45-degree line
  scale_x_continuous(limits = c(0, 5000)) +
  scale_y_continuous(limits = c(0, 5000)) +
  labs(title = "Model comparison", 
       x = "Observed", 
       y = "Predicted",
       color = "Model",
       shape = "Model") +
  theme_minimal()

#exploring data
#plot(m1)
#plot(m2)
#plot(null_model)
```

Our next plot will be for model 2, plotting predicted versus residuals (the latter being just residuals = predicted-observed). Also add a straight line at 0. Make sure your y-axis goes the same amount into the positive and negative direction.

The residuals have a pattern, a general negative to positive diagonal trend. This means the model isn't capturing some aspect of the data.

```{r}
#make a dataframe of only model2
m2df <- df_predictions[df_predictions$Model == "m2", ]
m2df$Residuals <- m2df$Predicted - m2df$Observed

#plot(m2, which = 1) #I have to do it using ggplot

ggplot(m2df, aes(x = Predicted, y = Residuals)) +
  geom_point(color = "blue", alpha = 0.6, size = 2) +  # Scatter points
  geom_hline(yintercept = 0, linetype = "solid", color = "red") +  # Line at y = 0 #red line at 0
  scale_y_continuous(limits = c(-4500, 4500)) +  # Symmetric y-axis
  scale_x_continuous(limits = c(-4500, 4500)) +  # X-axis limit
  labs(title = "Model 2", 
       x = "Observed", 
       y = "Predicted") +
  theme_minimal()
```

On to our next task, focusing on model 2 with 100 boostrapped values based on the training data.

Next, write a loop (or use a map or apply function) to fit the model to each of the bootstrap samples and make predictions from this model for the original training data. Record all predictions (e.g., in an array or a list).

Once you have all your predictions stored, compute the mean and confidence intervals.

Finally, make a figure that plots observed values on the x-axis, and point estimate (obtained from your original predictions on the training data), as well as median and the upper and lower bounds - obtained by the bootstrap sampling and stored in pred on the y-axis. You can for instance use black symbols for original predictions (the point estimate, which is the mean), and some colors to indicate median and lower and upper confidence limits. As above, make sure x- and y-axis are on the same and add a 45 degree line.

This plot of values looks better than in the earlier plots, the values and their 95% CIs look closer to the 45 degree line. It's hard to say if the light blue (original) or dark blue (bootstrapped) values are closer to the line, though, there's many cases of each instance.

```{r}
library(rsample)
set.seed(rngseed) #set seed

dat_bs <- bootstraps(train_data, times = 100) #get 100 bootstrap samples

#create boot to hold strapped values
boot <- matrix(0, nrow = length(dat_bs$splits), ncol = nrow(train_data))

# for each repetition, fit the model, predict, and store answer
for (i in 1:length(dat_bs$splits)) {
  dat_sample <- rsample::analysis(dat_bs$splits[[i]]) #sample for ith iteration
  m2_bs <- lm(Y ~ DOSE + AGE + SEX + WT + HT, data = dat_sample) #fit model to particular sample
  boot[i, ] <- predict(m2_bs, newdata = train_data) #make pred based on training data
}

#obtain CIs at 95% 
preds <- boot |> apply(2, quantile,  c(0.025, 0.5, 0.975)) |>  t()

#make it a dataframe to plot
preds_df <- data.frame(
  Observed = train_data$Y,
  Original_Pred = predict(m2, newdata = train_data),  # Original model predictions
  Lower_CI = preds[, 1],  # Lower confidence interval
  Median_CI = preds[, 2],  # Median
  Upper_CI = preds[, 3]   # Upper confidence interval
)

#plot observed vs. predictions with CIs
ggplot(preds_df, aes(x = Observed)) +
  geom_point(aes(y = Original_Pred), color = "skyblue3", shape = 16) + #original predictions
  geom_point(aes(y = Median_CI), color = "navy", shape = 16) + #median predictions
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), color = "firebrick", width = 0.1, alpha=0.6) + #CIs
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") + #45-degree line
  labs(x = "Observed", y = "Predicted") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 5000), ylim = c(0, 5000)) #axes go from 0 to 5000

```
