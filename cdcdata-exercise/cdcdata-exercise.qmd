---
title: "Data Analysis Excerise"
author: Prasanga Paudel
date: February 5, 2025
format:
  html:
    toc: false
    number-sections: true
    highlight-style: github
---


```{r, echo=FALSE, message=FALSE}
# load a few R packages
library(here)
library(knitr)
library(dslabs) # loading dslabs package as we will use gapminder dataset from it
library(tidyverse) # loading tidyverse package
library(readxl) #loading readxl package

```

{{< pagebreak >}}

# Introduction

 This is an excercise performed as a requirement for course Modern Applied Data Analytics. 
 
 _This exercise is based on CDC dataset._


# Data Acquisition
In this section we will acquire data from the directory, and have an initial look at it. This data on tax burden across different state was obtained from the annual compendium on tobacco revenue and industry statistics. These datasets are reported on an annual basis, and they include federal and state-level information regarding taxes applied to the price of a pack of cigarettes.
The data was last updated on March 22, 2021.

The dataset can be assessed from this link: <https://data.cdc.gov/Policy/Table-of-Gross-Cigarette-Tax-Revenue-Per-State-Orz/rkpp-igza/about_data>

{{< pagebreak >}}

# Importing Data

We will now import the data from our repository using the here command. We can observe that there are 2550 observations with 14 variables. The variables some observations can be viewed using the head() command.

```{r}
# Constructing the file path using here()
file_path <- here("cdcdata-exercise","cigarette-tax.xlsx")

# Importing the Excel file from the file path
cdc_dataset <- read_excel(file_path)

# Viewing the uppermost data of the imported dataset
head(cdc_dataset)
```


{{< pagebreak >}}

# Data Cleaning

```{r}
# Check for missing values within the cdc_dataset
summary(cdc_dataset)
str(cdc_dataset)
```

We have one (continuous) numeric variable in out dataset under that name "Data_Value". This variable actually represents the dollar amount of the revenue generated from cigarette sales. Another variable "Year" which represents the year of data collection has also been coded as numeric. The rest of the variables are coded as character as they have alphabets as values including the TopicID and MeasureId variables. The variable "LocationDesc" represents the name of the States.

## Checking for missing values
```{r}
# Check for missing values (NA) within the dataset
missing_values <- colSums(is.na(cdc_dataset))
print(missing_values) 
```

We can observe that there are 14 variables altogether and there are no missing values in the dataset. We will further check for any missing values coded as 9999 for any numeric variable. We will also use scatterplot to see if there is any such replacement for missing value which can be observed as an outlier. This can be observed later during the analysis.

## Confirming the structure of the dataset through observation across States
```{r}
# Counting the number of observations per State
state_counts <- cdc_dataset %>%
  count(LocationDesc)

# View the result
print(state_counts)

```
We can confirm that every state has 50 oservations for each State from year 1970-2019, which makes perfect sense.

{{< pagebreak >}}


#Preparing the dataset for replicable properties
As the raw data itself is quite messy, we will try to make it as simple as possible to learn about their distribution. As this is a time-series data, we will focus on the "trend" rather than the distribution as normal. We will try to clean the data as much as possible so that we can observe consisteny in the result, making it easier to learn about the properties of the dataset.

## Filtering out Five states to make the analysis easy.
We will filter five states from the dataset and make a new dataset based on these states, The next phases of analysis will include these states only:  Georgia, Kansas, Idaho, Alabama and Nebraska.


```{r}
# Filter dataset for selected states
filtered_states <- cdc_dataset %>%
  filter(LocationDesc %in% c("Georgia", "Kansas", "Idaho", "Alabama", "Nebraska"))

# View the first few rows of the new dataset
head(filtered_states)

```


## Filtering out the variables of interest
In this section, we will filter out only five variables so that we can look through the dataset more easily. We will set the name to this dataset as "dataset".


```{r}
# Select only the required columns
dataset <- filtered_states %>%
  select(LocationDesc, LocationAbbr, Year, Data_Value, GeoLocation)

# View the first few rows of the new dataset
head(dataset)

```

{{< pagebreak >}}


# Visualizing the data
We will use multiple techniques to visualize and summarize the dataset so that it will be easy for other classmates to understand the structure behind the data..

## Scatter plot of Revenue across time for our five states.
 We will first use scatter plot to observe the property of our numeric variable Data_Value.
 
```{r}

# Defining the custom colors for each state so it is easy for us to observe
custom_colors <- c("Georgia" = "red", 
                   "Kansas" = "blue", 
                   "Idaho" = "green", 
                   "Nebraska" = "purple", 
                   "Alabama" = "orange")

# Scatterplot of Year vs Data_Value
ggplot(dataset, aes(x = Year, y = Data_Value, color = LocationDesc)) +
  geom_point(alpha = 0.6, size = 2) +
  scale_color_manual(values = custom_colors) +
  theme_minimal() +
  labs(title = "Scatterplot of Data_Value Over Time for Selected States",
       x = "Year",
       y = "Data Value",
       color = "State") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

As there is a sudden break in the pattern after the year 2000, we will remove these years from our dataset and create a new dataset based on years 1970-2000. 

{{< pagebreak >}}


# Further Cleaning of the dataset

We will create a new datset based on year 1970 to 2000 which because they show somewhat consistent trend among the data_valuei.e revenue value
```{r}
# Filtering dataset for years less than 2001
data <- dataset %>% filter(Year < 2001)

# Viewing the first few rows of the newly filtered dataset
head(data)
```

We can observe that we now have 155 observations with 5 variables.
```{r}
# Counting the number of observations per state again.
state_counts2 <- data %>%
  count(LocationDesc)

# Viewing the count result
print(state_counts2)
```
Here, we can observe we have 31 observations for each State across 1970-2000

{{< pagebreak >}} 


# Association of variables in final Dataset
This section will talk about the association of revenue within the five state across the year 1970-2000. We will use few figures and a regression analysis to get some idea of the relationship.


## Scatter plot of Revenue across time for our five states. 
```{r}
ggplot(data, aes(x = Year, y = Data_Value, #setting the axes
                 color = LocationDesc)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +  # Adding a linear trend line
  theme_minimal() +
  labs(title = "Trend of Revenue from Cigarette(in US$) Over Years by State",
       x = "Year",
       y = "Revenue Value",
       color = "State")

``` 
This figure clearly shows the relation between the variables across time and States. This relation will be more clear if we use gregression analysis to calculate the slope. We will do that in next step. The scatter plot also confirms that there is no such missing value as encoded as 9999 otherwise we would have seen it as an outlier in our scatterplots.

## Calculating Slope or association through Regression

We will now try to express the relation of revenue over time for Georgia.
```{r}
# Filtering data for Georgia
georgia_data <- dataset %>% filter(LocationDesc == "Georgia")

# Running linear regression based on georgia
georgia_model <- lm(Data_Value ~ Year, data = georgia_data)

# Viewing regression summary for goergia
summary(georgia_model)
```

Furthermore, we will also express the relation for Alabama.
```{r}
# Filtering data for alabama
alabama_data <- dataset %>% filter(LocationDesc == "Alabama")

# Runing linear regression for alabama dataset
alabama_model <- lm(Data_Value ~ Year, data = alabama_data)

# Viewing regression summary for alabama model
summary(alabama_model)
```

From the two regression models above,  the relation can be clearly observed and the properties of the datasets are clearly expressed.

## Summary of Revenue across the five states of the final dataset

We will will now provide the full statistical summary of revenue based on the States present in our dataset including the mean, meadian, maximum and the minimums.

```{r}
# Summarizing Data Value for five selected states
summary_data <- data %>%
  filter(LocationDesc %in% c("Georgia", "Kansas", "Idaho", "Alabama", "Nebraska")) %>%
  group_by(LocationDesc) %>%
  summarise(
    count = n(), # provides no. of observation
    mean = mean(Data_Value, na.rm = TRUE), # provides mean
    median = median(Data_Value, na.rm = TRUE), #provides meadian
    min = min(Data_Value, na.rm = TRUE), # provides minimum value
    max = max(Data_Value, na.rm = TRUE), #provides max value
    sd = sd(Data_Value, na.rm = TRUE) # provides standard deviation
  )

# Viewing the summary result
print(summary_data)
```

## Summary of variables
This table provides the overall summary of the final datset.

```{r}
summary(data)
```

This section contributed by Mohammed Zuber


```{r setup, include=FALSE}
# Set global options for knitr
knitr::opts_chunk$set(echo = TRUE)
```

## Load Required Libraries
```{r}
# Load necessary libraries for data processing and visualization
library(readxl)  # For reading Excel files
library(dplyr)   # For data manipulation
library(ggplot2) # For visualization

# Set seed for reproducibility
set.seed(42)
```

## Load the Original Data
```{r}
# Define the path to the dataset
# Ensure the dataset is in the working directory

data_path <- "cigarette-tax.xlsx"

# Read the dataset from the specified sheet in the Excel file
df <- read_excel(data_path, sheet = "cigarette-tax")

# Display the first few rows to understand the dataset structure
head(df)
```
We will now import the data from our repository using the here command. We can observe that there are 2550 observations with 14 variables. The variables some observations can be viewed using the head() command.

## Generate Synthetic Data- LLM AI tools were used to help write the code
```{r}
# Copy the original dataset to preserve its structure
synthetic_df <- df

# Generate synthetic values for the 'Data_Value' column
# The values are randomly sampled within the original range
synthetic_df$Data_Value <- sample(
  min(df$Data_Value, na.rm = TRUE):max(df$Data_Value, na.rm = TRUE), 
  size = nrow(df), 
  replace = TRUE
)

# Display the first few rows of the synthetic dataset to verify changes
head(synthetic_df)
```
I created a synthetic dataset by copying the structure of the original dataset and replacing the Data_Value column with randomly sampled values within its original range. This ensures that the synthetic data maintains a similar distribution while removing any real-world patterns. While the minimum and maximum values are preserved, the randomness may not reflect trends or correlations present in the original data. 



# Data Cleaning for Synthetic Data

```{r}
# Check for missing values and summary statistics
summary(synthetic_df)

# Display the structure of the synthetic dataset
str(synthetic_df)
```
To assess the quality of the synthetic dataset, I checked for missing values and generated summary statistics using the summary() function. This provides key metrics such as the minimum, maximum, median, and quartiles, allowing for a comparison with the original dataset. Additionally, the str() function was used to confirm the structure and data types of each column, ensuring consistency with the original dataset. This step helps verify that the synthetic data retains the expected format and completeness before further analysis.


```{r}
# Check for missing values (NA) within the synthetic dataset
missing_values <- colSums(is.na(synthetic_df))

# Print the number of missing values for each column
print(missing_values)
```
To ensure data completeness, I checked for missing values in the synthetic dataset using colSums(is.na(synthetic_df)), which calculates the number of NA values in each column.
## Confirming the Structure of the Dataset Across States
```{r}
# Counting the number of observations per State
state_counts <- synthetic_df %>% count(LocationDesc)

# View the result
print(state_counts)
```
To assess the distribution of observations across states in the synthetic dataset, I utilized the count() function from the dplyr package. This function tallies the number of occurrences for each unique value in the specified column, in this case, LocationDesc. By executing synthetic_df %>% count(LocationDesc), I obtained a summary of how many records pertain to each state. 
## Filtering Five States for Further Analysis
```{r}
# Filter dataset for selected states
filtered_states <- synthetic_df %>%
  filter(LocationDesc %in% c("Georgia", "Kansas", "Idaho", "Alabama", "Nebraska"))

# View the first few rows of the new dataset
head(filtered_states)
```
To streamline the analysis, I filtered the dataset to include only five selected states: Georgia, Kansas, Idaho, Alabama, and Nebraska. This was done using the filter() function from dplyr, which extracts rows where the LocationDesc column matches one of the specified states. 
## Selecting Relevant Variables
```{r}
# Select only the required columns
dataset <- filtered_states %>%
  select(LocationDesc, LocationAbbr, Year, Data_Value, GeoLocation)

# View the first few rows of the new dataset
head(dataset)
```
To focus on key information, I selected only the most relevant variables from the filtered dataset using the select() function from dplyr. The chosen columns—LocationDesc (state name), LocationAbbr (state abbreviation), Year, Data_Value (cigarette sales/revenue), and GeoLocation (state coordinates)—are essential for analyzing trends over time.

## Scatter Plot of Revenue Across Time for Selected States
```{r}
# Define custom colors for each state
custom_colors <- c("Georgia" = "red", 
                   "Kansas" = "blue", 
                   "Idaho" = "green", 
                   "Nebraska" = "purple", 
                   "Alabama" = "orange")

# Scatterplot of Year vs Data_Value
ggplot(dataset, aes(x = Year, y = Data_Value, color = LocationDesc)) +
  geom_point(alpha = 0.6, size = 2) +
  scale_color_manual(values = custom_colors) +
  theme_minimal() +
  labs(title = "Scatterplot of Data_Value Over Time for Selected States",
       x = "Year",
       y = "Data Value",
       color = "State") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
The key difference between my scatter plot and my classmate’s is that mine appears randomly distributed, while theirs shows a clear trend over time, with values increasing before 2000 and declining afterward. This is because I generated my synthetic data using random sampling, which removed any underlying structure present in the original dataset. My classmate’s data retains a progressive pattern, suggesting that Data_Value follows a time-dependent trend. 
## Further Cleaning: Filtering Data for 1970-2000
```{r}
# Filtering dataset for years less than 2001
data <- dataset %>% filter(Year < 2001)

# Viewing the first few rows of the newly filtered dataset
head(data)
```
To focus on historical trends, I filtered the dataset to include only records from years before 2001 using the filter() function. This step ensures that the analysis captures a consistent time period, especially if later years exhibit different trends or irregularities. By restricting the data to 1970-2000, I aim to observe stable patterns without the potential disruptions seen in later years.

## Confirming Observations Per State
```{r}
# Counting the number of observations per state again.
state_counts2 <- data %>% count(LocationDesc)

# Viewing the count result
print(state_counts2)
```
To verify the distribution of observations across states after filtering, I used the count() function again to tally the number of records per state in the refined dataset. This step ensures that all selected states (Georgia, Kansas, Idaho, Alabama, and Nebraska) maintain a balanced representation after restricting the data to years before 2001. 

## Association of Variables: Scatter Plot with Trend Line
```{r}
ggplot(data, aes(x = Year, y = Data_Value, color = LocationDesc)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +  # Adding a linear trend line
  theme_minimal() +
  labs(title = "Trend of Revenue from Cigarette(in US$) Over Years by State",
       x = "Year",
       y = "Revenue Value",
       color = "State")
```
The key difference between my plot and my classmate’s is that my trend lines appear flatter, while theirs show a clear upward trajectory over time. This is because my synthetic Data_Value was generated randomly, removing any inherent growth patterns present in the original dataset. My classmate’s plot maintains a structured increase, indicating that revenue values naturally rise over time rather than fluctuating randomly.


## Regression Analysis: Relationship Over Time
```{r}
# Filtering data for Georgia
georgia_data <- dataset %>% filter(LocationDesc == "Georgia")

# Running linear regression based on Georgia
georgia_model <- lm(Data_Value ~ Year, data = georgia_data)

# Viewing regression summary for Georgia
summary(georgia_model)
```
The dataset maintains the original structure, with each selected state having 50 observations, ensuring consistency. However, the regression analysis for Georgia shows a very low R-squared value (0.00164) and a high p-value (0.7798), indicating no clear trend over time. This suggests that the synthetic Data_Value was randomly assigned and does not reflect a structured increase or decrease.
```{r}
# Filtering data for Alabama
alabama_data <- dataset %>% filter(LocationDesc == "Alabama")

# Running linear regression for Alabama dataset
alabama_model <- lm(Data_Value ~ Year, data = alabama_data)

# Viewing regression summary for Alabama model
summary(alabama_model)
```
The regression analysis for Alabama shows a low R-squared value (0.0078) and a high p-value (0.5419), indicating that Year does not significantly explain the variation in Data_Value. The near-zero R-squared suggests that the synthetic revenue values do not follow a clear increasing or decreasing trend over time. This likely results from the random assignment of Data_Value, which does not capture the expected real-world growth or decline.
## Summary Statistics of Revenue for Selected States
```{r}
# Summarizing Data Value for five selected states
summary_data <- data %>%
  group_by(LocationDesc) %>%
  summarise(
    count = n(), 
    mean = mean(Data_Value, na.rm = TRUE), 
    median = median(Data_Value, na.rm = TRUE), 
    min = min(Data_Value, na.rm = TRUE), 
    max = max(Data_Value, na.rm = TRUE), 
    sd = sd(Data_Value, na.rm = TRUE)
  )

# Viewing the summary result
print(summary_data)
```
The summary statistics for the five selected states indicate that each state has 31 observations, maintaining a balanced dataset. However, the mean and median values show high variability, suggesting inconsistencies in how Data_Value was generated. The wide range between min and max values and high standard deviation indicate that the synthetic data may still be too random rather than following a structured trend.
## Final Summary of the Dataset
```{r}
summary(data)
```
From the outputs, my synthetic dataset maintains the correct structure with selected variables such as LocationDesc, Year, and Data_Value, ensuring consistency. However, the regression analysis for Georgia and Alabama shows very low R-squared values (0.0016 and 0.0078, respectively) and high p-values (0.7798 and 0.5419), indicating no significant relationship between Year and Data_Value. Additionally, while the mean and median values for each state appear reasonable, the wide range and high standard deviation suggest excessive randomness in Data_Value generation. 




