---
title: "tidytuesday Exercise"
author: Prasanga Paudel
date: April 11, 2025
format:
  html:
    toc: false
    number-sections: true
    highlight-style: github
---


# Summary

Quantitattive metrics of healthcare quality offer vital information about hospital performance and patient outcomes.The `care_state` dataset from TidyTuesday (April 8), which includes state-level hospital care quality metrics for various medical diseases, is analysed in this exercise.  The dataset will provide an understanding of healthcare performance in the US by incorporating characteristics like patient scores, admission circumstances, measurement kinds, and time periods.

# Loading the libraries

```{r}
library(here)
library(dplyr)
```


# Importing dataset

We will import the dataset directly from Github using readr.

```{r}
care_state <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-08/care_state.csv')
```

Now, we will look at the uppermost observations from tha dataset.

```{r}
head(care_state)
```

The variables are coded in the correct way. But we will modify them according to our needs if required.

# Exploratory Data Analysis

The dates look interesting, we might be able to use the duration from start to end as an information in our research. We will first look if there are multiple start dates and end dates or if all the dates are same.

```{r}
#counting the occurrences of each start_date
care_state %>%
  count(start_date, name = "frequency") %>%
  arrange(desc(frequency)) %>%  # sorting by most frequent
  print(n = Inf)  #showing all rows

```

We can observe that there are only four start dates although some of them have very high frequency and some of them have a low frequency.

```{r}
#counting the occurrences of each end_date
care_state %>%
  count(end_date, name = "frequency") %>%
  arrange(desc(frequency)) %>%  #sorting by most frequent
  print(n = Inf)  #showing all rows
```

There are only two end dates.

So there is not much information in this variables for exploration.
 
 
## Checking the most frequent condition in each state

Now we will see which of the condition is most frequent in each of the state and if there is a pattern among the states.

```{r}
library(tidyverse)
library(usmap)

#finding most frequent condition per state
top_conditions <- care_state %>%
  count(state, condition) %>%
  group_by(state) %>%
  slice_max(n, n = 1) %>%
  ungroup()

#creating basic map
plot_usmap(data = top_conditions, values = "condition") +
  scale_fill_brewer(palette = "Set2", 
                   name = "Most Common Condition",
                   na.value = "gray") +
  labs(title = "Most Frequent Admission Condition by State") +
  theme(legend.position = "right")
```

We can see that for every state Emergency is the most frequent condition.

## Checking the frequencies of different measures.


Now, we will observe the freqency of different values under the variable `measure_name`.

```{r}

#getting frequency counts for measure_name
measure_counts <- care_state %>%
  count(measure_name, sort = TRUE)  #sorting by most frequent

#showing the full table
print(measure_counts, n = Inf)  #n= all rows

```

We can observe that all the unique values have 56 observations, except the first value which has double observations.

## Average time patients spent in the emergency department before being sent home

Creating a dataset to plot "Average time patients spent in the emergency department before being sent home" across states.

```{r}
library(tidyverse)

#creating filtered dataframe
avgtime <- care_state %>%
  filter(str_detect(measure_name, 
                   regex("Average time patients spent in the emergency department before being sent home", 
                        ignore_case = TRUE)))
```


Plotting the "Average time patients spent in the emergency department before being sent home" across states.

```{r}
library(tidyverse)
library(usmap)

#calculating average time by state
state_avg_time <- avgtime %>% 
  group_by(state) %>%
  summarise(average_time_spent = mean(score, na.rm = TRUE)) %>%
  ungroup()

#creating the map visualization
plot_usmap(data = state_avg_time, values = "average_time_spent", color = "white") +
  scale_fill_viridis_c(
    name = "Average Time (minutes)", 
    option = "plasma",
    direction = -1,
    na.value = "grey90"
  ) +
  labs(
    title = "Average ER Wait Time Before Discharge",
    subtitle = "Time spent in emergency department before being sent home"
  ) +
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5)
  )

```

We can observe that there is variation across states with central US states showing a low time spent in emergency department before being sent home.



## Influenza Vaccination Rates Among Healthcare Workers across states.

```{r}
library(tidyverse)
library(usmap)

#filtering vaccination data
flu_vax <- care_state %>%
  filter(str_detect(measure_name, 
                   "Healthcare workers given influenza vaccination")) %>%
  group_by(state) %>%
  summarise(vaccination_rate = mean(score, na.rm = TRUE))

#creating the map 
plot_usmap(data = flu_vax, 
           values = "vaccination_rate",
           color = "white") +
  scale_fill_distiller(
    name = "Vaccination Rate (%)",
    palette = "Spectral",
    direction = -1,  # Red = low, Green = high
    na.value = "grey90",
    limits = c(0, 100)  # 0-100% scale
  ) +
  labs(title = "Influenza Vaccination Rates Among Healthcare Workers",
       subtitle = "Higher percentages indicate better coverage") +
  theme(legend.position = "right")
```

We an observe that there is variation across different states.

```{r}
condition_corr <- care_state %>%
  group_by(condition) %>%
  summarise(mean_score = mean(score, na.rm = TRUE)) %>%
  arrange(desc(mean_score))

ggplot(condition_corr, aes(x = reorder(condition, mean_score), y = mean_score)) +
  geom_col(fill = "steelblue") +
  geom_text(aes(label = round(mean_score, 1)), vjust = -0.5) +
  labs(title = "Average Score by Medical Condition",
       x = "Condition",
       y = "Average Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
We can observe that medical condition and scores are also related to each other.


```{r}
library(ggplot2)

#days to numeric
care_state <- care_state %>% 
  mutate(days = as.numeric(end_date - start_date))

#creatig the plot days vs score
ggplot(care_state, aes(x = days, y = score)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Correlation Between Score and Duration (Days)",
       x = "Duration (Days)", 
       y = "Score") +
  theme_minimal()
```

The duration days also show some relation with the score.


# Hypothesis

We observed that there is a significant relation between states and scores. Also, we observed that days, measure name and conditions also show some relation with the score.

We hypothesize that the factors "measure name" "days", "state" and "condition" influence the score.

#  Analysis 

We observed that the start date and end dates do not vary much across the observations and are quite similar. There are four categories of start date and two categories of end date. We will create a variable "days" using the start date and the end date.

```{r}
care_state <- care_state %>%
  mutate(days = as.numeric(end_date - start_date))
```
We have now created the dataset.

Now, we will prepare the dataset for our analysis
```{r}
#preparing the modeling dataset
data <- care_state %>%
  select(score, days, state, condition, measure_name) %>%
  mutate(
    days = as.numeric(days),  # numeric
    across(c(state, condition, measure_name), as.factor)  #converting categoricals to factors
  ) %>%
  drop_na()  #removing rows with missing values
```


Splitting the data into train/train.

```{r}
library(tidymodels)  #rsample for splitting

#performing the 75/25 split
set.seed(123)  #for reproducibility
data_split <- initial_split(data, prop = 3/4)

#creating the training and train sets
train_data <- training(data_split)
test_data <- testing(data_split)
```

We will now use "days", "state", "condition", "measure_name" to predict the "score". We will use "Regression", "Lasso" and "Random Forest" models in the next sections.


## Regression

In this setion, we will perform a linear reression analysis. Although it is not a good idea to use variables with too many categories, but in this dataset the variables are too complex to categorize into smaller groups, so will proceed as it is.

```{r}
library(tidymodels)

#fitting linear regression
lm_fit <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(score ~ days + state + condition + measure_name, 
      data = train_data)

#getting model summary
tidy(lm_fit) %>% 
  arrange(p.value)  # Show most significant predictors first

#making predictions on train data
train_results <- train_data %>% 
  bind_cols(predict(lm_fit, new_data = train_data))

#evaluating performance
metrics <- train_results %>% 
  metrics(truth = score, estimate = .pred)

#showing performance metrics
print(metrics)

# Observed vs Predicted plot
ggplot(train_results, aes(x = score, y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, linetype = "dashed") +
  labs(title = "Linear Regression Performance",
       x = "Observed Scores", 
       y = "Predicted Scores") +
  theme_minimal()
```

We can observe that the RMSE value is 34.9 and the adjusted-R-square value is 0.876. The observed-predicted plot also shows a good fit for the datapoints.

## LASSO regression

```{r}
library(tidymodels)
library(glmnet)  #Lasso implementation

#setting up Lasso model specification
lasso_spec <- linear_reg(
  penalty = 0.1,  #Regularization strength 
  mixture = 1     # 1 = Lasso
) %>% 
  set_engine("glmnet")

#fitting Lasso model
lasso_fit <- lasso_spec %>% 
  fit(score ~ days + state + condition + measure_name, 
      data = train_data)

# coefficients (non-zero ones only)
tidy(lasso_fit) %>% 
  filter(estimate != 0) %>% 
  arrange(desc(abs(estimate)))

#making predictions
train_results <- train_data %>% 
  bind_cols(predict(lasso_fit, new_data = train_data))

#evaluating performance
lasso_metrics <- train_results %>% 
  metrics(truth = score, estimate = .pred)

#print result
cat("Lasso Performance:\n")
print(lasso_metrics)

#Observed vs Predicted plot
ggplot(train_results, aes(x = score, y = .pred)) +
  geom_point(alpha = 0.5, color = "darkorange") +
  geom_abline(slope = 1, linetype = "dashed") +
  labs(title = "Lasso Regression Performance",
       subtitle = paste("Penalty =", lasso_fit$spec$args$penalty),
       x = "Observed Scores", 
       y = "Predicted Scores") +
  theme_minimal()
```

We can observe a RMSE value of 35, which is smaller than what we observed for the OLS model. The R-squared value is 0.875 which is quite simillar to the regression model. The observed-predicted plot also shows a good fit as observed for the regression model.

## Random Forest

```{r}
library(tidymodels)
library(ranger)  #  random forest

#setting up and fit Random Forest
rf_spec <- rand_forest(
  mtry = 3,  #
  trees = 500,
  min_n = 5
) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

rf_fit <- rf_spec %>% 
  fit(score ~ days + state + condition + measure_name, 
      data = train_data)

#extracting variable importance 
importance_df <- rf_fit$fit$variable.importance %>% 
  enframe(name = "variable", value = "importance") %>% 
  arrange(desc(importance))

#plotting importance 
ggplot(importance_df, aes(x = reorder(variable, importance), y = importance)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(x = NULL, y = "Importance", 
       title = "Random Forest Variable Importance") +
  theme_minimal()

```
The random forest model shows a relatively high importance for measure_name, which makes sense.

```{r}
#making predictions and evaluate
rf_results <- train_data %>% 
  bind_cols(predict(rf_fit, new_data = train_data))

rf_metrics <- rf_results %>% 
  metrics(truth = score, estimate = .pred)

#comparing all models
bind_rows(
  linear_reg = metrics,
  lasso = lasso_metrics,
  random_forest = rf_metrics,
  .id = "model"
) %>% 
  select(model, .metric, .estimate) %>% 
  pivot_wider(names_from = .metric, values_from = .estimate)

```
We can observe that the RMSE of a random forest model is 27.06 while the R-square is 0.927. If we compare the three models, the Random Forest model has the lowest RMSE and the highest R-squared value. Thus RF performs the best.

```{r}
# Observed vs Predicted plot
ggplot(rf_results, aes(x = score, y = .pred)) +
  geom_point(alpha = 0.6, color = "darkgreen") +
  geom_abline(slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Random Forest: Observed vs Predicted",
       x = "Actual Score", y = "Predicted Score") +
  theme_minimal()
```

We can observe that the random forest model shows a very closer difference between the observed and the predicted. Hence, it is a better model.

# CV approach

we will now use a Cross-validation approach to test our results.

```{r}
#creating CV folds from training data (10-fold by default)
set.seed(123)
cv_folds <- vfold_cv(train_data, v = 5)  # 5-fold CV

#defining recipe (preprocessing)
model_recipe <- recipe(score ~ days + state + condition + measure_name, 
                      data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%  # Convert factors to dummy variables
  step_normalize(all_numeric_predictors())  # Center/scale numeric predictors

#model specifications
# Linear Regression
lm_spec <- linear_reg() %>% 
  set_engine("lm")

# Lasso Regression
lasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>% 
  set_engine("glmnet")

# Random Forest
rf_spec <- rand_forest(mtry = 3, trees = 500, min_n = 5) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

# Workflows
lm_wf <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(lm_spec)

lasso_wf <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(lasso_spec)

rf_wf <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(rf_spec)

#fitting Models with CV
lm_res <- fit_resamples(
  lm_wf,
  resamples = cv_folds,
  metrics = metric_set(rmse, rsq, mae)
)

lasso_res <- fit_resamples(
  lasso_wf,
  resamples = cv_folds,
  metrics = metric_set(rmse, rsq, mae)
)

rf_res <- fit_resamples(
  rf_wf,
  resamples = cv_folds,
  metrics = metric_set(rmse, rsq, mae)
)


```


Comparing Model Performance ----

```{r}
collect_metrics(lm_res)
```

We can observe that the RMSE values for the Linear regressions are quite consistent and show a comparitively low standard deviation. These RMSE values are also quite similar to the ones we obtained in our whole train dataset which was 34.9

```{r}
collect_metrics(lasso_res)
```

We can observe that the RMSE values for the LASSO regressions are also quite consistent and show a comparitively low standard deviation. These RMSE values are also quite similar to the ones we obtained in our whole train dataset which was 35. The R-squared values are also quite similar to our previous value.

```{r}
collect_metrics(rf_res)
```

In the case of the Random Forest model, we can observe that there is a very high increase in the RMSE value, almost the double of the previous value. This implies that the model performance of our Random forest model in the whole train dataset can not be trusted. We will now see if this pattern of inconsistency also exists in the test dataset or not.


# Test dataset
 
## Regression (Test dataset)

```{r}
#making predictions on test data
test_results <- test_data %>% 
  bind_cols(predict(lm_fit, new_data = test_data))

#  performance
test_metrics <- test_results %>% 
  metrics(truth = score, estimate = .pred)

#  performance metrics
print(test_metrics)

```
We can observe that the RMSE value for the linear regression has increased by almost 50% but the R-square value is somewhat consistent with the train results.


```{r}
# Observed vs Predicted plot
ggplot(test_results, aes(x = score, y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, linetype = "dashed") +
  labs(title = "Linear Regression Performance",
       x = "Observed Scores", 
       y = "Predicted Scores") +
  theme_minimal()
```

The predicted-observed plot shows a good fit for the datapoints.


## LASSO (TEST dataset)

Now, we will perfom the LASSO regression on the test dataset.

```{r}
#predictions
test_results <- test_data %>% 
  bind_cols(predict(lasso_fit, new_data = test_data))

#performance
lasso_metrics_test <- test_results %>% 
  metrics(truth = score, estimate = .pred)

#print performance
cat("Lasso Performance (Test data):\n")
print(lasso_metrics_test)

```

The LASSO regression also shows an almost 50% increase in RMSE values however the RMSE values are slightly greater than the Linear regression model although the difference is very small almost dismissable.


```{r}
# Observed vs Predicted plot
ggplot(test_results, aes(x = score, y = .pred)) +
  geom_point(alpha = 0.5, color = "darkorange") +
  geom_abline(slope = 1, linetype = "dashed") +
  labs(title = "Lasso Regression Performance on test data",
       subtitle = paste("Penalty =", lasso_fit$spec$args$penalty),
       x = "Observed Scores", 
       y = "Predicted Scores") +
  theme_minimal()
```


We can observe that the predicted-observed plot show a good fit for the datapoints.

## Random Forest (Test dataset)


```{r}
#  predictions 
rf_results_test <- test_data %>% 
  bind_cols(predict(rf_fit, new_data = test_data))

rf_metrics_test <- rf_results_test %>% 
  metrics(truth = score, estimate = .pred)

#comparing all three models
bind_rows(
  linear_reg = test_metrics,
  lasso = lasso_metrics_test,
  random_forest = rf_metrics_test,
  .id = "model"
) %>% 
  select(model, .metric, .estimate) %>% 
  pivot_wider(names_from = .metric, values_from = .estimate)

```

We can observe that the  RMSE value for the random forest is even bigger, increasing by almost 100%, the R-squared value has also decreased.

```{r}
# Observed vs Predicted plot
ggplot(rf_results_test, aes(x = score, y = .pred)) +
  geom_point(alpha = 0.6, color = "darkgreen") +
  geom_abline(slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Random Forest: Observed vs Predicted",
       x = "Actual Score", y = "Predicted Score") +
  theme_minimal()
```

We can observe that the random model also shows agood fit for the datapoints.

# Comparision Table

```{r}
#creating comprehensive comparison table with fixed CV metrics
performance_summary <- bind_rows(
  #training results
  bind_rows(
    linear_reg = metrics,
    lasso = lasso_metrics,
    random_forest = rf_metrics,
    .id = "model"
  ) %>% mutate(data = "Training"),
  
  # CV results
  bind_rows(
    linear_reg = show_best(lm_res, metric = "rmse") %>% slice(1) %>% mutate(.metric = "rmse"),
    linear_reg = show_best(lm_res, metric = "rsq") %>% slice(1) %>% mutate(.metric = "rsq"),
    linear_reg = show_best(lm_res, metric = "mae") %>% slice(1) %>% mutate(.metric = "mae"),
    lasso = show_best(lasso_res, metric = "rmse") %>% slice(1) %>% mutate(.metric = "rmse"),
    lasso = show_best(lasso_res, metric = "rsq") %>% slice(1) %>% mutate(.metric = "rsq"),
    lasso = show_best(lasso_res, metric = "mae") %>% slice(1) %>% mutate(.metric = "mae"),
    random_forest = show_best(rf_res, metric = "rmse") %>% slice(1) %>% mutate(.metric = "rmse"),
    random_forest = show_best(rf_res, metric = "rsq") %>% slice(1) %>% mutate(.metric = "rsq"),
    random_forest = show_best(rf_res, metric = "mae") %>% slice(1) %>% mutate(.metric = "mae"),
    .id = "model"
  ) %>% 
    select(model, mean, .metric) %>% 
    rename(.estimate = mean) %>% 
    mutate(data = "CV"),
  
  # Test results
  bind_rows(
    linear_reg = test_metrics,
    lasso = lasso_metrics_test,
    random_forest = rf_metrics_test,
    .id = "model"
  ) %>% mutate(data = "Test")
) %>% 
  select(model, data, .metric, .estimate) %>% 
  pivot_wider(names_from = .metric, values_from = .estimate) %>% 
  arrange(model, data)

#printing the formatted table
performance_summary %>% 
  gt::gt() %>% 
  gt::fmt_number(columns = c(rmse, rsq, mae), decimals = 2) %>% 
  gt::tab_header(
    title = "Model Performance Summary",
    subtitle = "Comparison across training, cross-validation and test sets"
  )
```


# Conclusion

Based on the results we can conclude that the both the Linear regression model and the LASSO model show a very similar result and can be considered same. Although the Random Forest model showed a better performance in the train model, the CV results and the results from the test dataset speaks against the RF model. Therefore, we will conclude that Linear regression is a better model although the LASSO also shows an equally competititive result.
